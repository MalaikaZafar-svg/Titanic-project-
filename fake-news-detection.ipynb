{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f44b8a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-08T16:14:47.651642Z",
     "iopub.status.busy": "2025-03-08T16:14:47.651250Z",
     "iopub.status.idle": "2025-03-08T16:14:50.839400Z",
     "shell.execute_reply": "2025-03-08T16:14:50.838365Z"
    },
    "papermill": {
     "duration": 3.193375,
     "end_time": "2025-03-08T16:14:50.841331",
     "exception": false,
     "start_time": "2025-03-08T16:14:47.647956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b28bd1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T16:14:50.848008Z",
     "iopub.status.busy": "2025-03-08T16:14:50.847513Z",
     "iopub.status.idle": "2025-03-08T16:14:50.856760Z",
     "shell.execute_reply": "2025-03-08T16:14:50.855360Z"
    },
    "papermill": {
     "duration": 0.014074,
     "end_time": "2025-03-08T16:14:50.858819",
     "exception": false,
     "start_time": "2025-03-08T16:14:50.844745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Change to the correct dataset path\n",
    "nltk.data.path.append(\"/kaggle/input/basic-nlp-with-nltk/basic-nlp-submission.csv \")  \n",
    "#Preprocessing \n",
    "def preprocess_text(text):\n",
    "    #lowercase the text\n",
    "    text = text.lower ()\n",
    "    #Remove punctuation \n",
    "    text = (str.maketrans('', '', string.punctuation)) \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    #Remove stopwords\n",
    "    stop_words = set(stop_words.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words] \n",
    "    #Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens =[Lemmatizer.lemmatize(word) for word in tokens] \n",
    "    #Join tokens\n",
    "    cleaned_text = ''.join(tokens)\n",
    "    return cleaned_text\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\"/kaggle/input/fake-news-detection-datasets/News _dataset/Fake.csv\")\n",
    "    print(\"Original Data:\")\n",
    "    print(df.head())\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    #Split dataset\n",
    "    X = df['cleaned_text']\n",
    "    Y = df ['label']\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2,random_state=42)\n",
    "    #Feature representation \n",
    "    vertorizer = CountVectorizer()\n",
    "    X_train_bow = vertorizer.fit_transform(X_test)\n",
    "    #Logistic Regression model\n",
    "    model = LogisticRegression ()\n",
    "    model.fit(X_train_bow, Y_train)\n",
    "    #Evaluate Model\n",
    "    Y_pred = model.predict(X_test_bow)\n",
    "    print(\"\\nModel Evaluation\")\n",
    "    print(\"Accuracy:\",accuracy_score(Y_test,Y_pred ))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(Y_test, Y_pred))\n",
    "    #Save model and vectorize\n",
    "    joib.dump(model, fake_news_model. pkl)\n",
    "    joblib.dump(vectorizer,'vectorizer.pkl')\n",
    "    print(\"n\\Model and vectorizer saved! \")\n",
    "    # To load the model later:\n",
    "    model = joblib.load('fake_news_model.pkl')\n",
    "    vectorizer = joblib.load('vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2712039,
     "sourceId": 4679796,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 1831367,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 5266785,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.884025,
   "end_time": "2025-03-08T16:14:51.581658",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-08T16:14:44.697633",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
